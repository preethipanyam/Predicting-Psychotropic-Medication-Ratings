{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5cf8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39384c9a",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a496b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = pd.read_csv('../Data/Processed_Data/MergedFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0e1163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>compound1</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <th>Neurodevelopmental Disorders</th>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <th>Personality Disorders</th>\n",
       "      <th>Psychotic Disorders</th>\n",
       "      <th>Trauma-related Disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>took</th>\n",
       "      <th>tried</th>\n",
       "      <th>week</th>\n",
       "      <th>weeks</th>\n",
       "      <th>weight</th>\n",
       "      <th>went</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.9172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231364</td>\n",
       "      <td>0.466684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.2148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500136</td>\n",
       "      <td>0.249187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>0.253855</td>\n",
       "      <td>0.183790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  compound1  Anxiety Disorders  Bipolar Disorders  \\\n",
       "0     8.0     1.9172                  0                  0   \n",
       "1    10.0     0.0911                  0                  1   \n",
       "2    10.0     1.0147                  0                  0   \n",
       "3    10.0     0.2148                  0                  1   \n",
       "4    10.0     0.4327                  0                  0   \n",
       "\n",
       "   Depressive Disorders  Neurodevelopmental Disorders  \\\n",
       "0                     0                             1   \n",
       "1                     0                             0   \n",
       "2                     1                             0   \n",
       "3                     0                             0   \n",
       "4                     0                             1   \n",
       "\n",
       "   Obsessive-Compulsive Disorders  Personality Disorders  Psychotic Disorders  \\\n",
       "0                               0                      0                    0   \n",
       "1                               0                      0                    0   \n",
       "2                               0                      0                    0   \n",
       "3                               0                      0                    0   \n",
       "4                               0                      0                    0   \n",
       "\n",
       "   Trauma-related Disorders  ...  took     tried      week  weeks    weight  \\\n",
       "0                         0  ...   0.0  0.231364  0.466684    0.0  0.000000   \n",
       "1                         0  ...   0.0  0.328339  0.000000    0.0  0.000000   \n",
       "2                         0  ...   0.0  0.000000  0.000000    0.0  0.000000   \n",
       "3                         0  ...   0.0  0.000000  0.000000    0.0  0.000000   \n",
       "4                         0  ...   0.0  0.486605  0.000000    0.0  0.500136   \n",
       "\n",
       "       went  work    worked      year     years  \n",
       "0  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "1  0.336281   0.0  0.000000  0.000000  0.000000  \n",
       "2  0.000000   0.0  0.000000  0.000000  0.320926  \n",
       "3  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "4  0.249187   0.0  0.261021  0.253855  0.183790  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4765774a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30918, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5d407",
   "metadata": {},
   "source": [
    "### Combining similar vectors (proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f465b49",
   "metadata": {},
   "source": [
    "I noticed (retrospectively) that a few of my features were TFIDF generated vectors that were very similar in meaning (i.e. same words different tense); I researched how TFIDF vector scores were calculated, and decided to combine these features into one so it serves as a proxy for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71adaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych['years_tf'] = psych['year'] + psych['years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae6399c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych['week_tf'] = psych['week'] + psych['weeks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0dc22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych['days_tf'] = psych['day'] + psych['days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych['feel_tf'] = psych['feel'] + psych['feeling'] + psych['felt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2017f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = psych.drop(columns = ['year', 'years', 'week', 'weeks', 'day', 'days'\n",
    "                             ,'feel', 'feeling', 'felt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34ede933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30918, 74)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c765b16",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e3f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = psych.drop(columns = 'Rating', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86974b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = psych['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc40e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4097192",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dacefa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ccac6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.347076073831292, 0.3486416558861578)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)\n",
    "mnb.score(X_train, y_train), mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6fef7",
   "metadata": {},
   "source": [
    "The scores are very low (0.35 for both train & test). I'm curious to see if doing a simple logistic regression might be a better direction to take:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324c7d0",
   "metadata": {},
   "source": [
    "### Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41d2ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'liblinear', penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba5119f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37083836467138176, 0.3714100905562743)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_train, y_train), logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98aa449",
   "metadata": {},
   "source": [
    "The scores for the logistic regression are a little better than the Naive Bayes model (0.37 for both train & test), but low. <br><br> After doing some research, based on my features it may be best to try a Random Forest model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e717fd1",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c4f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(max_features = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f667404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9335657692326066, 0.5628603782102022)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_train, y_train)\n",
    "rfr.score(X_train, y_train), rfr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e78a4",
   "metadata": {},
   "source": [
    "Model is very overfit; the train accuracy is good but test accuracy is very low. I will try using a RandomForestClassifier instead; all of my predictors/features were initially categorical to begin with (prior to transforming/dummifying) so it made sense to have a categorical target as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94c546",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (10 class prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd3b5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_features = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b26ca1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9965499396239434, 0.635058214747736)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450577c1",
   "metadata": {},
   "source": [
    "This model performed slightly better than the Random Forest Regressor but it is still very overfit (train accuracy almost 1 but test accuracy is pretty low at 0.64). Will try testing different hyperparemters to see if this helps:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f4f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth = 24, max_features = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d988af3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9589874072796274, 0.6159120310478654)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac5cf5",
   "metadata": {},
   "source": [
    "Seems like the scores are similar to the first RFC model, with 0.96 for train and 0.62 for test; I will try inceasing the number of n_estimators from the default (100) to 1000 to see if this improves my model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb55b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, max_features = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d0240cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9624805934103847, 0.6216041397153945)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ae64d",
   "metadata": {},
   "source": [
    "The model performed about the same as the previous even after adding more n_estimators (0.96 train & 0.62 test). After doing some research I decided to add class_weight as a hyperparameter to help deal with the imbalanced classes in my target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa77c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, \n",
    "                             max_features = 9, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10ba1b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.977315853027428, 0.6213454075032342)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d68666",
   "metadata": {},
   "source": [
    "The train score improved by a little bit but the test score is the same (0.98 train, 0.62 test); will try adding the oob_score hyperparameter as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ede440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, \n",
    "                             max_features = 9, class_weight = 'balanced',\n",
    "                            oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "621252a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9781783681214421, 0.6200517464424321)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234a690",
   "metadata": {},
   "source": [
    "Model has the same results (0.98 train, 0.62 test); the last few models I've run have all been pretty overfit. Will try adding the 'criterion' hyperparameter to see if it helps with the overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce97cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, \n",
    "                             max_features = 9, class_weight = 'balanced',\n",
    "                            oob_score = True, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2a04acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.985423494911161, 0.6316946959896507)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a81b91",
   "metadata": {},
   "source": [
    "The accuracy score for train went up from 0.98 to 0.99, and test went up by 0.01 as well (0.62 to 0.63); the model is still overfit. <br><br> I will try reclassifying the target variable as 3 classes (low, medium, & high) to see if this helps better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40479846",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (3 class prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e64b6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['Rating'] <= 3:\n",
    "        return 1\n",
    "    elif row['Rating'] <= 7:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae61452",
   "metadata": {},
   "source": [
    "**Note**: 'Low' ratings classified is 1-3, 'Medium' as 4-7 and 'High' as 8 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e1d5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "psych['Rating_cat'] = psych.apply(f, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd5d6db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    20245\n",
       "1     5357\n",
       "2     5316\n",
       "Name: Rating_cat, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych['Rating_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9d8e8",
   "metadata": {},
   "source": [
    "Looks like the classes are quite imbalanced, with about 65% belonging to the 'High' rating class. Will definitely need to account for that in my RFC model by including the class_weight hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b523d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>compound1</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <th>Neurodevelopmental Disorders</th>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <th>Personality Disorders</th>\n",
       "      <th>Psychotic Disorders</th>\n",
       "      <th>Trauma-related Disorders</th>\n",
       "      <th>Anxiolytics</th>\n",
       "      <th>B vitamin</th>\n",
       "      <th>MAOI</th>\n",
       "      <th>NRDI</th>\n",
       "      <th>SARI</th>\n",
       "      <th>SNRI</th>\n",
       "      <th>SSRI</th>\n",
       "      <th>SSRI/5HT-1A Partial Agonist</th>\n",
       "      <th>TeCA</th>\n",
       "      <th>alpha agonists</th>\n",
       "      <th>anticonvulsants</th>\n",
       "      <th>antihistamines</th>\n",
       "      <th>antihypertensives</th>\n",
       "      <th>antimanic agents</th>\n",
       "      <th>anxiolytics</th>\n",
       "      <th>atypical antipsychotics</th>\n",
       "      <th>benzodiazepines</th>\n",
       "      <th>beta blockers</th>\n",
       "      <th>central nervous system stimulants</th>\n",
       "      <th>conventional antipsychotics</th>\n",
       "      <th>tricyclic antidepressants</th>\n",
       "      <th>ago</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>attacks</th>\n",
       "      <th>bad</th>\n",
       "      <th>better</th>\n",
       "      <th>depression</th>\n",
       "      <th>did</th>\n",
       "      <th>didnt</th>\n",
       "      <th>doctor</th>\n",
       "      <th>dont</th>\n",
       "      <th>dose</th>\n",
       "      <th>drug</th>\n",
       "      <th>effects</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>help</th>\n",
       "      <th>helped</th>\n",
       "      <th>im</th>\n",
       "      <th>ive</th>\n",
       "      <th>just</th>\n",
       "      <th>life</th>\n",
       "      <th>like</th>\n",
       "      <th>medication</th>\n",
       "      <th>medicine</th>\n",
       "      <th>months</th>\n",
       "      <th>night</th>\n",
       "      <th>panic</th>\n",
       "      <th>prescribed</th>\n",
       "      <th>really</th>\n",
       "      <th>sleep</th>\n",
       "      <th>started</th>\n",
       "      <th>taking</th>\n",
       "      <th>time</th>\n",
       "      <th>took</th>\n",
       "      <th>tried</th>\n",
       "      <th>weight</th>\n",
       "      <th>went</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>years_tf</th>\n",
       "      <th>week_tf</th>\n",
       "      <th>days_tf</th>\n",
       "      <th>feel_tf</th>\n",
       "      <th>Rating_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.9172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183442</td>\n",
       "      <td>0.164529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466684</td>\n",
       "      <td>0.650502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288118</td>\n",
       "      <td>0.251365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257267</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258745</td>\n",
       "      <td>0.505013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236771</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372802</td>\n",
       "      <td>0.325247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302118</td>\n",
       "      <td>0.356197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306363</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.2148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400823</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187110</td>\n",
       "      <td>0.201708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233165</td>\n",
       "      <td>0.192908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486605</td>\n",
       "      <td>0.500136</td>\n",
       "      <td>0.249187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>0.437645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  compound1  Anxiety Disorders  Bipolar Disorders  \\\n",
       "0     8.0     1.9172                  0                  0   \n",
       "1    10.0     0.0911                  0                  1   \n",
       "2    10.0     1.0147                  0                  0   \n",
       "3    10.0     0.2148                  0                  1   \n",
       "4    10.0     0.4327                  0                  0   \n",
       "\n",
       "   Depressive Disorders  Neurodevelopmental Disorders  \\\n",
       "0                     0                             1   \n",
       "1                     0                             0   \n",
       "2                     1                             0   \n",
       "3                     0                             0   \n",
       "4                     0                             1   \n",
       "\n",
       "   Obsessive-Compulsive Disorders  Personality Disorders  Psychotic Disorders  \\\n",
       "0                               0                      0                    0   \n",
       "1                               0                      0                    0   \n",
       "2                               0                      0                    0   \n",
       "3                               0                      0                    0   \n",
       "4                               0                      0                    0   \n",
       "\n",
       "   Trauma-related Disorders  Anxiolytics  B vitamin  MAOI  NRDI  SARI  SNRI  \\\n",
       "0                         0            0          0     0     0     0     0   \n",
       "1                         0            0          0     0     0     0     0   \n",
       "2                         0            0          1     0     0     0     0   \n",
       "3                         0            0          0     0     0     0     0   \n",
       "4                         0            0          0     0     0     0     0   \n",
       "\n",
       "   SSRI  SSRI/5HT-1A Partial Agonist  TeCA  alpha agonists  anticonvulsants  \\\n",
       "0     0                            0     0               1                0   \n",
       "1     0                            0     0               0                0   \n",
       "2     0                            0     0               0                0   \n",
       "3     0                            0     0               0                1   \n",
       "4     0                            0     0               0                0   \n",
       "\n",
       "   antihistamines  antihypertensives  antimanic agents  anxiolytics  \\\n",
       "0               0                  0                 0            0   \n",
       "1               0                  0                 0            0   \n",
       "2               0                  0                 0            0   \n",
       "3               0                  0                 0            0   \n",
       "4               0                  0                 0            0   \n",
       "\n",
       "   atypical antipsychotics  benzodiazepines  beta blockers  \\\n",
       "0                        0                0              0   \n",
       "1                        1                0              0   \n",
       "2                        0                0              0   \n",
       "3                        0                0              0   \n",
       "4                        0                0              0   \n",
       "\n",
       "   central nervous system stimulants  conventional antipsychotics  \\\n",
       "0                                  0                            0   \n",
       "1                                  0                            0   \n",
       "2                                  0                            0   \n",
       "3                                  0                            0   \n",
       "4                                  1                            0   \n",
       "\n",
       "   tricyclic antidepressants  ago   anxiety  attacks  bad    better  \\\n",
       "0                          0  0.0  0.000000      0.0  0.0  0.203022   \n",
       "1                          0  0.0  0.000000      0.0  0.0  0.288118   \n",
       "2                          0  0.0  0.000000      0.0  0.0  0.372802   \n",
       "3                          0  0.0  0.279727      0.0  0.0  0.000000   \n",
       "4                          0  0.0  0.000000      0.0  0.0  0.000000   \n",
       "\n",
       "   depression       did  didnt    doctor  dont      dose  drug   effects  \\\n",
       "0    0.000000  0.227612    0.0  0.213254   0.0  0.233223   0.0  0.000000   \n",
       "1    0.251365  0.000000    0.0  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "2    0.325247  0.000000    0.0  0.000000   0.0  0.000000   0.0  0.314417   \n",
       "3    0.000000  0.000000    0.0  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "4    0.000000  0.000000    0.0  0.224258   0.0  0.000000   0.0  0.180062   \n",
       "\n",
       "       good     great  help  helped        im      ive  just      life  \\\n",
       "0  0.227833  0.000000   0.0     0.0  0.000000  0.00000   0.0  0.000000   \n",
       "1  0.000000  0.000000   0.0     0.0  0.257267  0.00000   0.0  0.258745   \n",
       "2  0.000000  0.000000   0.0     0.0  0.000000  0.00000   0.0  0.000000   \n",
       "3  0.000000  0.435728   0.0     0.0  0.000000  0.36248   0.0  0.000000   \n",
       "4  0.000000  0.000000   0.0     0.0  0.000000  0.00000   0.0  0.000000   \n",
       "\n",
       "       like  medication  medicine   months  night  panic  prescribed  really  \\\n",
       "0  0.000000    0.000000  0.000000  0.00000    0.0    0.0    0.000000     0.0   \n",
       "1  0.505013    0.000000  0.000000  0.00000    0.0    0.0    0.347716     0.0   \n",
       "2  0.326724    0.000000  0.000000  0.36691    0.0    0.0    0.000000     0.0   \n",
       "3  0.330040    0.000000  0.400823  0.00000    0.0    0.0    0.000000     0.0   \n",
       "4  0.187110    0.201708  0.000000  0.00000    0.0    0.0    0.000000     0.0   \n",
       "\n",
       "      sleep   started    taking      time  took     tried    weight      went  \\\n",
       "0  0.000000  0.183442  0.164529  0.000000   0.0  0.231364  0.000000  0.000000   \n",
       "1  0.000000  0.260331  0.000000  0.000000   0.0  0.328339  0.000000  0.336281   \n",
       "2  0.000000  0.000000  0.302118  0.356197   0.0  0.000000  0.000000  0.000000   \n",
       "3  0.411276  0.000000  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   \n",
       "4  0.233165  0.192908  0.000000  0.000000   0.0  0.486605  0.500136  0.249187   \n",
       "\n",
       "   work    worked  years_tf   week_tf   days_tf   feel_tf  Rating_cat  \n",
       "0   0.0  0.000000  0.000000  0.466684  0.650502  0.000000           3  \n",
       "1   0.0  0.000000  0.000000  0.000000  0.000000  0.236771           3  \n",
       "2   0.0  0.000000  0.320926  0.000000  0.000000  0.306363           3  \n",
       "3   0.0  0.000000  0.000000  0.000000  0.402206  0.000000           3  \n",
       "4   0.0  0.261021  0.437645  0.000000  0.186126  0.000000           3  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84fdbd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30918, 75)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "90933b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = psych.drop(columns = ['Rating', 'Rating_cat'])\n",
    "y = psych['Rating_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7691cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ccb2839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, \n",
    "                             max_features = 9, class_weight = 'balanced',\n",
    "                            oob_score = True, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "981746e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9797740210453683, 0.811901681759379)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114dcac",
   "metadata": {},
   "source": [
    "The RFC model predicting 3 classes of Drug Ratings instead of 10 classes is doing much better. The accuracy score of train is still high (0.98) but the test accuracy went up to 0.81, which signifies that the model is the least overfit of all the RFC models thus far. Though it is still a bit overfit, a test accuracy of 0.81 is decent.<br><br> Was also curious to try changing class weight from 'balanced' to 'balanced_sibsample' to see if this helps the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "68979991",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, \n",
    "                             max_features = 9, class_weight = 'balanced_subsample',\n",
    "                            oob_score = True, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "650269e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9798602725547697, 0.8124191461836999)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train), rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11fdee",
   "metadata": {},
   "source": [
    "Looks like the results are comprable to the model above (0.98 train and 0.81 test). This model is pretty good and seems to be my best model thus far so will select this model as my final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0692cf8",
   "metadata": {},
   "source": [
    "### Final Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1bd29e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e7645e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8124191461836999"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "91aa649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70661308, 0.5822156 , 0.88272642])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred2, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46f8f6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7238517008421833"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred2, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e869db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7997869148706447"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred2, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792873fd",
   "metadata": {},
   "source": [
    "The model metrics I will likely report/am most intersted in for this model are the f1 score for the 'low' & 'high' category (0.71, 0.88 respectively)as well as the weighted f1 score, which is the average of the f1 scores of all 3 categories but with class weight factored in (0.80). Overall, the f1 scores are pretty good, suggesting the model can successfully predict the class of low and high drug ratings (which is what physicians & drug manufacturers may be most intersted in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5883abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "993ba9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm2, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbccea0",
   "metadata": {},
   "source": [
    "### Comparing Metrics to Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "49366de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7730,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50c7f3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "26701777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "baseline = np.full([7730], 3, dtype = int)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "211fdf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6527813712807244"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d01ceee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.7899186])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, baseline, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c69fe6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2633061991233563"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, baseline, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b5c3cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51564414519138"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, baseline, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a862a36",
   "metadata": {},
   "source": [
    "The baseline metrics in comparison to my model metrics (weighted f1 score 0.52 for baseline compared to 0.80 for my final model) suggest that my model is performing well in comparison to the baseline model and that it can accurately predict drug rating from the given predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b92e8",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f75be",
   "metadata": {},
   "source": [
    "My final model will be the Random Forest Classifier predicting 3 classes of Ratings (low, medium, & high). The final model is restated in a mardown cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baaaf81",
   "metadata": {},
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 1000, max_depth = 24, max_features = 9, class_weight = 'balanced_subsample',\n",
    "oob_score = True, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba904d5e",
   "metadata": {},
   "source": [
    "Overall, this model is pretty good but still a bit overfit. I can continue improving this model by working on the feature selection and feature engineering more, and perhaps tuning the hyperparameters differently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
